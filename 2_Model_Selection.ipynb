{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Selection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point forward, I began making heavy use of this asynchronous notebook running. As a result, I have a lot of notebooks that are essentially duplicates of each other with minor details tweaked. For each of these, I will go through the base process, and note the tweaked details along with links to the notebooks used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import model_processes as mp\n",
    "#sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the relevant data: The mp.frame_from_drive uses the SciDrive module from the SciServer package to load the given frame. The mp.frame_from_url does exactly the same thing, but with a URL acquired manually. URLs are public, so running this cell will work.  \n",
    "Both functions have, in addition to the path/url, two optional arguments. The first (grouped=bool, default True) grouped the target feature according to class, rather than subclass. The second (replace_9=bool, default=False) replaces -9.999 (the value used in the dataset to represent missing or bad data) with NaN wherever it appears in the feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = mp.frame_from_drive('SomeStars')\n",
    "csv_url = 'https://www.scidrive.org/vospace-2.0/data/1bab3dd0-0961-4776-9d12-52b908ae63d1'\n",
    "df = mp.frame_from_url(csv_url)\n",
    "X, y = df.iloc[:,1:].sort_index(axis=1), df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initial Models\n",
    "With my sampled data, I began trying out some initial models. Since I had the easy integrated access to the asynchronous notebook running, in [Notebook 1](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/2_1_MVP_Model.ipynb), I started by essentially throwing a bunch of models at my data and seeing how they did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_dict = {\n",
    "    'Logistic': LogisticRegression(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Linear SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=1000),\n",
    "    'Gradient Boost': GradientBoostingClassifier(n_estimators=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mp.batch_classify_models function takes a batch of classifiers along with X and y train and test sets, and returns 1. a dataframe generated from the classification report function, 2. a dictionary of confusion matrices, normalized by true class, keyed to classifier name, and 3. a dictionary of trained models, again keyed to classifier name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_models_performances = mp.batch_classify_models(classifiers_dict, X_train, y_train, X_test, y_test)\n",
    "initial_models_performances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_url = 'https://www.scidrive.org/vospace-2.0/data/2026e3f7-1222-4258-8660-c8b9fccea2b8'\n",
    "performance = pd.read_csv(performances_url, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not going to rerun this process here, but I saved the output of this first run in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = mp.performance_frame(initial_performances[0])\n",
    "scores = mp.performance_frame(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Logistic</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Linear SVM</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gradient Boost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th>Type</th>\n",
       "      <th>ClassSize</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <th>overall</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>0.848970</td>\n",
       "      <td>0.837656</td>\n",
       "      <td>0.705823</td>\n",
       "      <td>0.853801</td>\n",
       "      <td>0.520722</td>\n",
       "      <td>0.905034</td>\n",
       "      <td>0.903382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict_time</th>\n",
       "      <th>overall</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>0.720311</td>\n",
       "      <td>139.821869</td>\n",
       "      <td>21.121665</td>\n",
       "      <td>0.037039</td>\n",
       "      <td>175.449976</td>\n",
       "      <td>62.922563</td>\n",
       "      <td>15.311615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <th>overall</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>1460.293430</td>\n",
       "      <td>1.341346</td>\n",
       "      <td>12.396218</td>\n",
       "      <td>16.536355</td>\n",
       "      <td>853.176440</td>\n",
       "      <td>434.422161</td>\n",
       "      <td>61852.924313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">f1-score</th>\n",
       "      <th>A</th>\n",
       "      <th>1241.0</th>\n",
       "      <td>0.905306</td>\n",
       "      <td>0.869882</td>\n",
       "      <td>0.827532</td>\n",
       "      <td>0.917141</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.955260</td>\n",
       "      <td>0.956487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV</th>\n",
       "      <th>65.0</th>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.508475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CarbonWD</th>\n",
       "      <th>584.0</th>\n",
       "      <td>0.890026</td>\n",
       "      <td>0.890277</td>\n",
       "      <td>0.744526</td>\n",
       "      <td>0.844784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935652</td>\n",
       "      <td>0.922810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <th>1882.0</th>\n",
       "      <td>0.812839</td>\n",
       "      <td>0.786663</td>\n",
       "      <td>0.768831</td>\n",
       "      <td>0.834259</td>\n",
       "      <td>0.676626</td>\n",
       "      <td>0.881391</td>\n",
       "      <td>0.881374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <th>497.0</th>\n",
       "      <td>0.123128</td>\n",
       "      <td>0.463675</td>\n",
       "      <td>0.061962</td>\n",
       "      <td>0.604331</td>\n",
       "      <td>0.386648</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.662366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <th>1131.0</th>\n",
       "      <td>0.887621</td>\n",
       "      <td>0.874132</td>\n",
       "      <td>0.669145</td>\n",
       "      <td>0.865513</td>\n",
       "      <td>0.351332</td>\n",
       "      <td>0.909716</td>\n",
       "      <td>0.920469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT</th>\n",
       "      <th>161.0</th>\n",
       "      <td>0.394052</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510067</td>\n",
       "      <td>0.503356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <th>2209.0</th>\n",
       "      <td>0.967320</td>\n",
       "      <td>0.957442</td>\n",
       "      <td>0.795047</td>\n",
       "      <td>0.951059</td>\n",
       "      <td>0.563736</td>\n",
       "      <td>0.973447</td>\n",
       "      <td>0.974023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OB</th>\n",
       "      <th>96.0</th>\n",
       "      <td>0.659341</td>\n",
       "      <td>0.643275</td>\n",
       "      <td>0.293839</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757062</td>\n",
       "      <td>0.726257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>0.664721</td>\n",
       "      <td>0.676590</td>\n",
       "      <td>0.507044</td>\n",
       "      <td>0.705713</td>\n",
       "      <td>0.275371</td>\n",
       "      <td>0.801255</td>\n",
       "      <td>0.783957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>0.829386</td>\n",
       "      <td>0.831274</td>\n",
       "      <td>0.703493</td>\n",
       "      <td>0.854523</td>\n",
       "      <td>0.474031</td>\n",
       "      <td>0.902457</td>\n",
       "      <td>0.901391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">precision</th>\n",
       "      <th>A</th>\n",
       "      <th>1241.0</th>\n",
       "      <td>0.917287</td>\n",
       "      <td>0.875204</td>\n",
       "      <td>0.812743</td>\n",
       "      <td>0.924652</td>\n",
       "      <td>0.976581</td>\n",
       "      <td>0.955645</td>\n",
       "      <td>0.956487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV</th>\n",
       "      <th>65.0</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.566038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CarbonWD</th>\n",
       "      <th>584.0</th>\n",
       "      <td>0.886248</td>\n",
       "      <td>0.929236</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.836975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950530</td>\n",
       "      <td>0.934974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <th>1882.0</th>\n",
       "      <td>0.731210</td>\n",
       "      <td>0.744661</td>\n",
       "      <td>0.691883</td>\n",
       "      <td>0.830089</td>\n",
       "      <td>0.816679</td>\n",
       "      <td>0.861492</td>\n",
       "      <td>0.863405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <th>497.0</th>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.591522</td>\n",
       "      <td>0.626126</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.711316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <th>1131.0</th>\n",
       "      <td>0.881430</td>\n",
       "      <td>0.858483</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.868984</td>\n",
       "      <td>0.945736</td>\n",
       "      <td>0.885356</td>\n",
       "      <td>0.905128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT</th>\n",
       "      <th>161.0</th>\n",
       "      <td>0.490741</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.235043</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.554745</td>\n",
       "      <td>0.547445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <th>2209.0</th>\n",
       "      <td>0.963196</td>\n",
       "      <td>0.952509</td>\n",
       "      <td>0.950851</td>\n",
       "      <td>0.956502</td>\n",
       "      <td>0.392502</td>\n",
       "      <td>0.967785</td>\n",
       "      <td>0.972047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OB</th>\n",
       "      <th>96.0</th>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.190184</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.783133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>0.708173</td>\n",
       "      <td>0.744165</td>\n",
       "      <td>0.511408</td>\n",
       "      <td>0.707623</td>\n",
       "      <td>0.417514</td>\n",
       "      <td>0.838090</td>\n",
       "      <td>0.804441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>0.827448</td>\n",
       "      <td>0.832956</td>\n",
       "      <td>0.731591</td>\n",
       "      <td>0.855412</td>\n",
       "      <td>0.635237</td>\n",
       "      <td>0.902147</td>\n",
       "      <td>0.900399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">recall</th>\n",
       "      <th>A</th>\n",
       "      <th>1241.0</th>\n",
       "      <td>0.893634</td>\n",
       "      <td>0.864625</td>\n",
       "      <td>0.842869</td>\n",
       "      <td>0.909750</td>\n",
       "      <td>0.336019</td>\n",
       "      <td>0.954875</td>\n",
       "      <td>0.956487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV</th>\n",
       "      <th>65.0</th>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CarbonWD</th>\n",
       "      <th>584.0</th>\n",
       "      <td>0.893836</td>\n",
       "      <td>0.854452</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.852740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.921233</td>\n",
       "      <td>0.910959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <th>1882.0</th>\n",
       "      <td>0.914984</td>\n",
       "      <td>0.833688</td>\n",
       "      <td>0.865037</td>\n",
       "      <td>0.838470</td>\n",
       "      <td>0.577577</td>\n",
       "      <td>0.902232</td>\n",
       "      <td>0.900106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <th>497.0</th>\n",
       "      <td>0.074447</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.036217</td>\n",
       "      <td>0.617706</td>\n",
       "      <td>0.279678</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.619718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <th>1131.0</th>\n",
       "      <td>0.893899</td>\n",
       "      <td>0.890363</td>\n",
       "      <td>0.716180</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.215738</td>\n",
       "      <td>0.935455</td>\n",
       "      <td>0.936340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT</th>\n",
       "      <th>161.0</th>\n",
       "      <td>0.329193</td>\n",
       "      <td>0.155280</td>\n",
       "      <td>0.341615</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472050</td>\n",
       "      <td>0.465839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <th>2209.0</th>\n",
       "      <td>0.971480</td>\n",
       "      <td>0.962426</td>\n",
       "      <td>0.683115</td>\n",
       "      <td>0.945677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979176</td>\n",
       "      <td>0.976007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OB</th>\n",
       "      <th>96.0</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.677083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>0.652600</td>\n",
       "      <td>0.649699</td>\n",
       "      <td>0.563961</td>\n",
       "      <td>0.704635</td>\n",
       "      <td>0.267668</td>\n",
       "      <td>0.775140</td>\n",
       "      <td>0.767120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <th>7866.0</th>\n",
       "      <td>0.848970</td>\n",
       "      <td>0.837656</td>\n",
       "      <td>0.705823</td>\n",
       "      <td>0.853801</td>\n",
       "      <td>0.520722</td>\n",
       "      <td>0.905034</td>\n",
       "      <td>0.903382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Logistic         KNN  Naive Bayes  \\\n",
       "Metric       Type         ClassSize                                         \n",
       "accuracy     overall      7866.0        0.848970    0.837656     0.705823   \n",
       "predict_time overall      7866.0        0.720311  139.821869    21.121665   \n",
       "train_time   overall      7866.0     1460.293430    1.341346    12.396218   \n",
       "f1-score     A            1241.0        0.905306    0.869882     0.827532   \n",
       "             CV           65.0          0.342857    0.356436     0.124031   \n",
       "             CarbonWD     584.0         0.890026    0.890277     0.744526   \n",
       "             F            1882.0        0.812839    0.786663     0.768831   \n",
       "             G            497.0         0.123128    0.463675     0.061962   \n",
       "             K            1131.0        0.887621    0.874132     0.669145   \n",
       "             LT           161.0         0.394052    0.247525     0.278481   \n",
       "             M            2209.0        0.967320    0.957442     0.795047   \n",
       "             OB           96.0          0.659341    0.643275     0.293839   \n",
       "             macro avg    7866.0        0.664721    0.676590     0.507044   \n",
       "             weighted avg 7866.0        0.829386    0.831274     0.703493   \n",
       "precision    A            1241.0        0.917287    0.875204     0.812743   \n",
       "             CV           65.0          0.450000    0.500000     0.082902   \n",
       "             CarbonWD     584.0         0.886248    0.929236     0.796875   \n",
       "             F            1882.0        0.731210    0.744661     0.691883   \n",
       "             G            497.0         0.355769    0.494305     0.214286   \n",
       "             K            1131.0        0.881430    0.858483     0.627907   \n",
       "             LT           161.0         0.490741    0.609756     0.235043   \n",
       "             M            2209.0        0.963196    0.952509     0.950851   \n",
       "             OB           96.0          0.697674    0.733333     0.190184   \n",
       "             macro avg    7866.0        0.708173    0.744165     0.511408   \n",
       "             weighted avg 7866.0        0.827448    0.832956     0.731591   \n",
       "recall       A            1241.0        0.893634    0.864625     0.842869   \n",
       "             CV           65.0          0.276923    0.276923     0.246154   \n",
       "             CarbonWD     584.0         0.893836    0.854452     0.698630   \n",
       "             F            1882.0        0.914984    0.833688     0.865037   \n",
       "             G            497.0         0.074447    0.436620     0.036217   \n",
       "             K            1131.0        0.893899    0.890363     0.716180   \n",
       "             LT           161.0         0.329193    0.155280     0.341615   \n",
       "             M            2209.0        0.971480    0.962426     0.683115   \n",
       "             OB           96.0          0.625000    0.572917     0.645833   \n",
       "             macro avg    7866.0        0.652600    0.649699     0.563961   \n",
       "             weighted avg 7866.0        0.848970    0.837656     0.705823   \n",
       "\n",
       "                                     Decision Tree  Linear SVM  Random Forest  \\\n",
       "Metric       Type         ClassSize                                             \n",
       "accuracy     overall      7866.0          0.853801    0.520722       0.905034   \n",
       "predict_time overall      7866.0          0.037039  175.449976      62.922563   \n",
       "train_time   overall      7866.0         16.536355  853.176440     434.422161   \n",
       "f1-score     A            1241.0          0.917141    0.500000       0.955260   \n",
       "             CV           65.0            0.366667    0.000000       0.616822   \n",
       "             CarbonWD     584.0           0.844784    0.000000       0.935652   \n",
       "             F            1882.0          0.834259    0.676626       0.881391   \n",
       "             G            497.0           0.604331    0.386648       0.671875   \n",
       "             K            1131.0          0.865513    0.351332       0.909716   \n",
       "             LT           161.0           0.357143    0.000000       0.510067   \n",
       "             M            2209.0          0.951059    0.563736       0.973447   \n",
       "             OB           96.0            0.610526    0.000000       0.757062   \n",
       "             macro avg    7866.0          0.705713    0.275371       0.801255   \n",
       "             weighted avg 7866.0          0.854523    0.474031       0.902457   \n",
       "precision    A            1241.0          0.924652    0.976581       0.955645   \n",
       "             CV           65.0            0.400000    0.000000       0.785714   \n",
       "             CarbonWD     584.0           0.836975    0.000000       0.950530   \n",
       "             F            1882.0          0.830089    0.816679       0.861492   \n",
       "             G            497.0           0.591522    0.626126       0.754386   \n",
       "             K            1131.0          0.868984    0.945736       0.885356   \n",
       "             LT           161.0           0.342857    0.000000       0.554745   \n",
       "             M            2209.0          0.956502    0.392502       0.967785   \n",
       "             OB           96.0            0.617021    0.000000       0.827160   \n",
       "             macro avg    7866.0          0.707623    0.417514       0.838090   \n",
       "             weighted avg 7866.0          0.855412    0.635237       0.902147   \n",
       "recall       A            1241.0          0.909750    0.336019       0.954875   \n",
       "             CV           65.0            0.338462    0.000000       0.507692   \n",
       "             CarbonWD     584.0           0.852740    0.000000       0.921233   \n",
       "             F            1882.0          0.838470    0.577577       0.902232   \n",
       "             G            497.0           0.617706    0.279678       0.605634   \n",
       "             K            1131.0          0.862069    0.215738       0.935455   \n",
       "             LT           161.0           0.372671    0.000000       0.472050   \n",
       "             M            2209.0          0.945677    1.000000       0.979176   \n",
       "             OB           96.0            0.604167    0.000000       0.697917   \n",
       "             macro avg    7866.0          0.704635    0.267668       0.775140   \n",
       "             weighted avg 7866.0          0.853801    0.520722       0.905034   \n",
       "\n",
       "                                     Gradient Boost  \n",
       "Metric       Type         ClassSize                  \n",
       "accuracy     overall      7866.0           0.903382  \n",
       "predict_time overall      7866.0          15.311615  \n",
       "train_time   overall      7866.0       61852.924313  \n",
       "f1-score     A            1241.0           0.956487  \n",
       "             CV           65.0             0.508475  \n",
       "             CarbonWD     584.0            0.922810  \n",
       "             F            1882.0           0.881374  \n",
       "             G            497.0            0.662366  \n",
       "             K            1131.0           0.920469  \n",
       "             LT           161.0            0.503356  \n",
       "             M            2209.0           0.974023  \n",
       "             OB           96.0             0.726257  \n",
       "             macro avg    7866.0           0.783957  \n",
       "             weighted avg 7866.0           0.901391  \n",
       "precision    A            1241.0           0.956487  \n",
       "             CV           65.0             0.566038  \n",
       "             CarbonWD     584.0            0.934974  \n",
       "             F            1882.0           0.863405  \n",
       "             G            497.0            0.711316  \n",
       "             K            1131.0           0.905128  \n",
       "             LT           161.0            0.547445  \n",
       "             M            2209.0           0.972047  \n",
       "             OB           96.0             0.783133  \n",
       "             macro avg    7866.0           0.804441  \n",
       "             weighted avg 7866.0           0.900399  \n",
       "recall       A            1241.0           0.956487  \n",
       "             CV           65.0             0.461538  \n",
       "             CarbonWD     584.0            0.910959  \n",
       "             F            1882.0           0.900106  \n",
       "             G            497.0            0.619718  \n",
       "             K            1131.0           0.936340  \n",
       "             LT           161.0            0.465839  \n",
       "             M            2209.0           0.976007  \n",
       "             OB           96.0             0.677083  \n",
       "             macro avg    7866.0           0.767120  \n",
       "             weighted avg 7866.0           0.903382  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function transforms the confusion matrices into heatmaps, with the subplots sorted from top to bottom according to the unweighted average value of the correct classifications that lie along the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted(list(y_test.unique()))\n",
    "cms = mp.print_confusion_matrices(initial_models_performances[1], class_names, figsz=(6,4), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 More Models\n",
    "After this notebook, I ran [Notebook 2](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/2_2_More_Models.ipynb) with more models. Based on the comparative success of the tree based models from the first notebook, I tried out some more tree based models, listed in the dict below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_dict = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=1000),\n",
    "    'Random Forest Shallow': RandomForestClassifier(max_depth=6, n_estimators=1000),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=1000),\n",
    "    'Extra Trees Shallow': ExtraTreesClassifier(max_depth=6, n_estimators=1000),\n",
    "    'Hist Boosted': HistGradientBoostingClassifier(max_iter=1000),\n",
    "    'Hist Boosted Shallow': HistGradientBoostingClassifier(max_depth=6, max_iter=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from this batch:  \n",
    "<img src=\"pics/tree_models_cmf_mats.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Selection\n",
    "While this was running, I also started testing out the various combinations of features I put together in the previous notebook. Random Forest was the best performing model of the first batch, so I used that in [Notebook 3](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/2_3_Feature_Selection.ipynb), to evaluate each set of features. This notebook contains a process almost identical to the one above. The difference is that instead of calling mp.batch_classify_models on a classifier dict, it calls mp.batch_classify_data_subsets on a dict with path name keyed to a dataframe with the corresponding subset of features.\n",
    "The results: <img src='pics/feature_tuning_cmf_mats.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results (and some subsequent similar ones once I got [xgboost](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/2_3_Feature_Selection_xgb.ipynb) and [hgboost](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/2_3_Feature_Selection_Hist.ipynb) working), I ended up deciding not to eliminate any features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Getting XGBoost and HistGradientBoosting working\n",
    "I got stuck on this part for a while. Basically, when I submit a notebook to run as an asynchronous job, it starts with a new default environment every - an environment with sklearn version 0.22, which doesn't support NaN handling in HGBoost, and without XGBoost entirely. In the end, I figured out that I could add the following cells to the beginning of relevant notebooks to make them work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I got these models working, I ran the same processes as above on them. In addition, I tried out a little (super basic) parameter tuning. The exciting part (for me anyway) about this part was using the built in NaN handling for these models! Performance went up (by a lil bit) when I switched from the -9.999 values to NaNs. [Trying out hist](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/2_4_Trying_out_hist.ipynb) [Trying out xgb](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/2_4_Trying_out_xgb.ipynb)  \n",
    "<img src='pics/hist_xgb.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point was also when I started looking at balanced accuracy score as a metric of success - in the interest of having a singular success metric for the purposes of parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Balanced Accuracy Scores:  \n",
    "xgb = 0.77746346  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results I was definitely liking xgboost and hgboost the best, but was having a difficult time deciding between the two models, so I ended up continuing with both for a while."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
