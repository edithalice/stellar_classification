{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Parameter Tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's GridCV Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at this point, I was kind of oscillating between XGBoost and HistGradientBoosting. I was leaning towards XGBoost, so I did a bunch of parameter tuning on that, on the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {'max_depth':[2, 3, 4, 5, 6], 'gamma':[0, 0.01, 0.1]}\n",
    "grow_params = {'eta':[0.1, 0.2, 0.3, 0.4], 'grow_policy':['depthwise', 'lossguide'], \n",
    "               'eval_metric':['merror', 'mlogloss']} #n_estimators ++\n",
    "sample_params = {'subsample':[0.5, 0.6, 0.75, 0.9, 1], 'colsample_bytree':[0.8, 0.9, 1]}\n",
    "reg_params = {'lambda':[1, 1.2, 1.4], 'alpha':[0, 0.2, 0.4, 0.6]}\n",
    "weight_params = {'scale_pos_weight':[0, 0.25, 0.5, 0.75], 'min_child_weight':[1, 2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier({'objective':'multi:softmax', 'tree_method':'hist', 'num_class':9, \n",
    "                           'n_estimators':400, 'seed':42})\n",
    "clf = GridSearchCV(xgb_model, tree_params, scoring='balanced_accuracy', \n",
    "                   error_score=0, n_jobs=20)\n",
    "clf.fit(X, y)\n",
    "print(clf.cv_results_)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the above cell more or less identically for each of the above parameter dicts:  \n",
    "[tree_params](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/4_1_Parameter_Tuning_tree.ipynb),\n",
    "[grow_params](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/4_1_Parameter_Tuning_grow.ipynb),\n",
    "[sample_params](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/4_1_Parameter_Tuning_sample.ipynb),\n",
    "[reg_params](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/4_1_Parameter_Tuning_reg.ipynb),\n",
    "[weight_params](https://github.com/edithalice/stellar_classification/blob/master/notebook_runs/4_1_Parameter_Tuning_weight.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:  \n",
    "- min_child_weight=2\n",
    "- subsample=1\n",
    "- max_depth=6\n",
    "- gamma=0.1\n",
    "- col_sample_bytree=0.8\n",
    "- lambda=1.4\n",
    "- alpha=0  \n",
    "\n",
    "and looks like these params have no effect on the model:\n",
    "- eval_metric\n",
    "- grow_policy\n",
    "- scale_pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was going to do a second round based on the above results to tweak these a little more, but then! I tweaked a couple paramaters for HistGradientBoost and its performance jumped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Hist Gradient Boost in 270.549 seconds.\n",
    "Hist Gradient Boost predicted test data in 6.681 seconds. \n",
    "Trained XGBoost in 2317.085 seconds.  \n",
    "XGBoost predicted test data in 17.215 seconds.\n",
    "\n",
    "Hist Gradient Boost: 0.8459  \n",
    "XGBoost: 0.845\n",
    "<img src='pics/hist_v_xgb.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only slightly better than XGBoost, but much faster! At this point, I decided to go with the Hist Gradient Boosting model for sure, so I started doing some parameter tuning for that model. (Which was so much quicker since there a lot fewer parameters!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round 1\n",
    "tree_params = {'learning_rate':[0.1, 0.2, 0.3], 'max_depth':[4, 6, 8]}\n",
    "grad_params = {'max_iter':[200, 350, 500], 'l2_regularization':[0.6, 1, 1.4]}\n",
    "#round 2\n",
    "tree_params = {'learning_rate':[0.05, 0.1, 0.15], 'max_depth':[7, 8, 9]}\n",
    "grad_params = {'max_iter':[350, 500], 'l2_regularization':[1.3, 1.4, 1.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingClassifier(random_state=25)\n",
    "clf = GridSearchCV(model, tree_params, scoring='balanced_accuracy', error_score=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.cv_results_)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of round 1:\n",
    "- learning_rate=0.1\n",
    "- max_depth=8\n",
    "- max_iter=350\n",
    "- l2_regularization=1.4  \n",
    "\n",
    "aaaand results of round 2 were... exactly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingClassifier(random_state=25, learning_rate=0.1, max_depth=8,\n",
    "                                      max_iter=350, l2_regularization=1.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
